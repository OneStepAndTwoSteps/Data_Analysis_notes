### 多项式回归


如果我们的数据集难以使用直线来进行拟合，但是令人惊讶的是，依然可以使用线性模型来拟合非线性数据。

一个简单的方法是对 __每个特征进行加权后作为新的特征__ ，然后训练一个线性模型在这个扩展的特征集。这种方法称为 __多项式回归__

我们使用 Scikit-Learning 的	PolynomialFeatures	类进行训练数据集的转换，让训练集中每个特征的平方（2	次多项式） 作为新特征（在这种情况下，仅存在一个特征）：

#### PolynomialFeatures

__PolynomialFeatures__ 这个类有 3 个参数：

* degree：控制多项式的次数；

* interaction_only：默认为 False，如果指定为 True，那么就不会有特征自己和自己结合的项，组合的特征中没有 a2 和 b2；

* include_bias：默认为 True 。如果为 True 的话，那么结果中就会有 0 次幂项，即全为 1 这一列。

__参数详解：__

* interaction_only 的意思是，得到的组合特征只有相乘的项，没有平方项。

* interaction_only 设置成 True 的意思是： 例如 [a,b] 的多项式交互式输出 [1,a,b,ab]。

* include_bias 设置 0 次幂那一列是否要。

__构造数据__

    x = np.random.uniform(-3, 3, size=100)
    X = x.reshape(-1, 1)
    noise = np.random.normal(0, 1, size=100)
    noise = noise.reshape(-1, 1)
    y = 0.5 * X**2 + X + 2 + noise


__使用sklearn中的PolynomialFeatures进行多项式回归__

    from sklearn.preprocessing import PolynomialFeatures
    poly_features = PolynomialFeatures(degree=2,include_bias=False)
    x_poly = poly_features.fit_transform(X)

__X_poly__ 现在 __包含原始特征__ 和 __这个特征的平方__ 。

    print('X[0]: ',X[0])
    print('x_poly[0]: ',x_poly[0])

    X[0]:  [0.7637525]
    x_poly[0]:  [0.7637525  0.58331788]

现在你可以在这个扩展训练集上使用 __LinearRegression__ 模型进行拟合。

    lin_reg2 = LinearRegression()
    lin_reg2.fit(x_poly,y)
    
    LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False) 

__根据线性回归得到预测值__

    y_predect = lin_reg2.predict(x_poly)


#### 使用多项式拟合数据
    
__排序__ 是为了在绘制线的时候从最左端开始绘制

__y_predect[np.argsort(x)]__ 依据 __np.sort(x)__ 的顺序

    plt.scatter(x,y)
    plt.plot(np.sort(x), y_predect[np.argsort(x)], color='r')

<div align=center><img width="450" height="350" src="https://raw.githubusercontent.com/OneStepAndTwoSteps/Data_Analysis_notes/master/static/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E7%AE%97%E6%B3%95/1.jpg"/></div>

