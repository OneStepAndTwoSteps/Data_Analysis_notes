
## Xgboost

Xgboost是一种boosting集成算法(串行)，本身是由很多CART回归树集成。但是什么是CART树？

CART树分两种：

    分类树：

        分类树分析是指预测结果是数据所属的类（比如某个电影去看还是不看） 分类树用于离散数据

    回归树：

        回归树分析是指预测结果可以被认为是实数（例如房屋的价格，或患者在医院中的逗留时间）回归树用于连续数据

我们知道 __分类树__ 的节点分裂可以依靠信息增益（ID3）、信息增益率（C4.5）、基尼系数（CART）等等方法进行分裂判断(比如我们在判断一个人是男是女，我们可以通过判断是否为长头发来分类男生女生，但是我们也可以通过是否穿高更鞋，是否有喉结这些特征进行判断，信息增益（ID3）、信息增益率（C4.5）、基尼系数（CART）等等方法可以帮助我们选择最优的结果作为我们的分裂点)，但是对于 __回归树__，你没法再用分类树那套信息增益、信息增益率、基尼系数来判定树的节点分裂了，你需要采取新的方式评估效果，包括预测误差（常用的有均方误差、对数误差等）。


### GBDT

说到Xgboost，不得不先从GBDT(Gradient Boosting Decision Tree)说起。因为xgboost本质上还是一个GBDT，但是力争把速度和效率发挥到极致，所以叫X (Extreme) GBoosted。包括前面说过，两者都是boosting方法。

__GBDT的原理很简单，就是所有弱分类器的结果相加等于预测值__，然后 __下一个弱分类器去拟合误差函数对预测值的梯度(残差)__ 这个梯度(残差)就是预测值与真实值之间的误差。

__举个例子：__

假如现在我们想去银行贷款1000元，但计算机或者模型GBDT并不知道我想去银行贷款多少元，那GBDT咋办呢？

    1、它会在第一个弱分类器（或第一棵树中）随便用一个贷款金额比如950元来拟合，然后发现误差有50元；
    2、接下来在第二棵树中，用30元去拟合剩下的损失(第一个模型生成的残差1000-950=50元)，发现差距还有20元；
    3、接着在第三棵树中用15元拟合剩下的差距，发现差距只有5元了；
    4、最后在第四课树中用5元拟合剩下的残差.

最终，四棵树的结论加起来，就是银行贷款1000元。实际工程中，gbdt是计算负梯度，用负梯度近似残差。


### Xgboost

举个例子，我们要预测一家人对电子游戏的喜好程度，考虑到年轻和年老相比，年轻更可能喜欢电子游戏，以及男性和女性相比，男性更喜欢电子游戏，故先根据年龄大小区分小孩和大人，然后再通过性别区分开是男是女，逐一给各人在电子游戏喜好程度上打分，如下图所示。

<div align=center><img  src="https://raw.githubusercontent.com/OneStepAndTwoSteps/data_mining_analysis/master/static/xgboost/1.png"/></div>

就这样，训练出了2棵树tree1和tree2，类似之前gbdt的原理，两棵树的结论累加起来便是最终的结论，所以小孩的预测分数就是两棵树中小孩所落到的结点的分数相加：2 + 0.9 = 2.9。爷爷的预测分数同理：-1 + （-0.9）= -1.9。


现在我们可以看出xgboost与gbdt有着异曲同工之妙，一样也是通过将所有弱分类器的结果相加得到最终的预测值，但两者并不完全相同，两者的目标函数不一样。xgboost的目标函数如下图所示：

<div align=center><img  src="https://raw.githubusercontent.com/OneStepAndTwoSteps/data_mining_analysis/master/static/xgboost/6.png"/></div>


    一、1为我们t-1个模型的残差，2为我们的新加的树模型，整个l()就是我们的损失函数。

    二、3为我们的正则项（包括L1正则、L2正则）

    三、constant 是我们的常数项

额，突然丢这么大一个公式，不少人可能瞬间就懵了。没事，下面咱们来拆解下这个目标函数，并一一剖析每个公式、每个符号、每个下标的含义。

#### Xgboost的核心思想

    1、不断地添加树，不断地进行特征分裂来生长一棵树，每次添加一个树，其实是学习一个新函数，去拟合上次预测的残差。

    2、当我们训练完成得到k棵树，我们要预测一个样本的分数，其实就是根据这个样本的特征，在每棵树中会落到对应的一个叶子节点，每个叶子节点就对应一个分数。

    3、最后只需要将每棵树对应的分数加起来就是该样本的预测值。


#### 模型学习与训练误差

具体来说，目标函数第一部分中的 i 表示第个样本， l(^y^i − y^i ) 表示第 i 个样本的预测误差，我们的目标当然是误差越小越好。

类似之前GBDT的套路，xgboost也是需要将多棵树的得分累加得到最终的预测得分（每一次迭代，都在现有树的基础上，增加一棵树去拟合前面树的预测结果与真实值之间的残差）。


下图是xgboost的预测算法模型：

<div align=center><img  src="https://raw.githubusercontent.com/OneStepAndTwoSteps/data_mining_analysis/master/static/xgboost/2.png"/></div>


我们如何选择每一轮加入什么 f(树) 呢？答案是非常直接的，选取一个f(树) 来使得我们的目标函数尽量最大地降低。

<div align=center><img  src="https://raw.githubusercontent.com/OneStepAndTwoSteps/data_mining_analysis/master/static/xgboost/5.png"/></div>


上面那个Obj的公式表达的可能有些过于抽象，我们可以考虑当 是平方误差的情况（相当于 l(y^i,^y^i)=(^y^i-y^i)^2），这个时候我们的目标可以被写成下面这样的二次函数（图中画圈的部分表示的就是预测值和真实值之间的残差）：

<div align=center><img  src="https://raw.githubusercontent.com/OneStepAndTwoSteps/data_mining_analysis/master/static/xgboost/5-1.png"/></div>



#### 正则项：树的复杂度

首先，梳理下几个规则

    用叶子节点集合以及叶子节点得分表示 

    每个样本都落在一个叶子节点上 
    
    q(x)表示样本x在某个叶子节点上，wq(x)是该节点的打分，即该样本的模型预测值



我们也可以通过W_q(x)(叶子节点q的分数) 来代表一棵决策树模型，如下图所示：

<div align=center><img  src="https://raw.githubusercontent.com/OneStepAndTwoSteps/data_mining_analysis/master/static/xgboost/3.png"/></div>



但是有一点是，当我们树模型变多时，可能会给我们带来过拟合的效果，这里我们引入正则化项：

<div align=center><img  src="https://raw.githubusercontent.com/OneStepAndTwoSteps/data_mining_analysis/master/static/xgboost/4.png"/></div>


T表示叶子节点的个数，w表示叶子节点的分数。直观上看，目标要求预测误差尽量小，且叶子节点T尽量少（γ控制叶子结点的个数），节点数值w尽量不极端（λ控制叶子节点的分数不会过大），防止过拟合。

    T表示的是树里面叶子节点的个数

    w的L2模平方是对w进行L2正则化，相当于针对每个叶结点的得分增加L2平滑，目的是为了避免过拟合

最后我们可以得到目标函数：

<div align=center><img  src="https://raw.githubusercontent.com/OneStepAndTwoSteps/data_mining_analysis/master/static/xgboost/8.png"/></div>





### 分裂节点

之前我们说过回归树不在能使用分类树的分裂评估方法，所以接下去我们讨论如何进行回归树的分裂。

对于一个叶子节点如何进行分裂，xgboost作者在其原始论文中给出了两种分裂节点的方法：

1）枚举所有不同树结构的贪心法

现在的情况是只要知道树的结构，就能得到一个该结构下的最好分数，那如何确定树的结构呢？

一个想当然的方法是：不断地枚举不同树的结构，然后利用打分函数来寻找出一个最优结构的树，接着加入到模型中，不断重复这样的操作。而再一想，你会意识到要枚举的状态太多了，基本属于无穷种，那咋办呢？

我们试下贪心法，从树深度0开始，每一节点都遍历所有的特征，比如年龄、性别等等，然后对于某个特征，先按照该特征里的值进行排序，然后线性扫描该特征进而确定最好的分割点，最后对所有特征进行分割后，我们选择所谓的增益Gain最高的那个特征，而Gain如何计算呢？

目标函数中的G/(H+λ)部分，表示着每一个叶子节点对当前模型损失的贡献程度，融合一下，得到Gain的计算表达式，如下所示：

<div align=center><img  src="https://raw.githubusercontent.com/OneStepAndTwoSteps/data_mining_analysis/master/static/xgboost/7.png"/></div>


第一个值得注意的事情是“对于某个特征，先按照该特征里的值进行排序”，这里举个例子。

比如设置一个值a，然后枚举所有x < a、a  < x这样的条件（x代表某个特征比如年龄age，把age从小到大排序：假定从左至右依次增大，则比a小的放在左边，比a大的放在右边），对于某个特定的分割a，我们要计算a左边和右边的导数和。

比如总共五个人，按年龄排好序后，一开始我们总共有如下4种划分方法：

把第一个人和后面四个人划分开
把前两个人和后面三个人划分开
把前三个人和后面两个人划分开
把前面四个人和后面一个人划分开

接下来，把上面4种划分方法全都各自计算一下Gain，看哪种划分方法得到的Gain值最大则选取哪种划分方法，经过计算，发现把第2种划分方法“前面两个人和后面三个人划分开”得到的Gain值最大，意味着在一分为二这个第一层层面上这种划分方法是最合适的。
换句话说，对于所有的特征x，我们只要做一遍从左到右的扫描就可以枚举出所有分割的梯度和GL和GR。然后用计算Gain的公式计算每个分割方案的分数就可以了。

然后后续则依然按照这种划分方法继续第二层、第三层、第四层、第N层的分裂。

第二个值得注意的事情就是引入分割不一定会使得情况变好，所以我们有一个引入新叶子的惩罚项。优化这个目标对应了树的剪枝， 当引入的分割带来的增益小于一个阀值γ 的时候，则忽略这个分割。

换句话说，当引入某项分割，结果分割之后得到的分数 - 不分割得到的分数得到的值太小（比如小于我们的最低期望阀值γ），但却因此得到的复杂度过高，则相当于得不偿失，不如不分割。即做某个动作带来的好处比因此带来的坏处大不了太多，则为避免复杂 多一事不如少一事的态度，不如不做。

相当于在我们发现“分”还不如“不分”的情况下后（得到的增益太小，小到小于阈值γ），会有2个叶子节点存在同一棵子树上的情况。

2）近似算法

主要针对数据太大，不能直接进行计算


-《[笔记参考来源](https://blog.csdn.net/v_july_v/article/details/81410574)》

