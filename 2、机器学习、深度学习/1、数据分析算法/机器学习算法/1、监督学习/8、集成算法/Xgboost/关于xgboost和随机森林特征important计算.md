# 关于xgboost和随机森林特征important计算

随机森林和xgboost中的importance是一样的计算方法。

概括地说，一棵树中，某个特征的importance就是损失函数在这个特征切分点上的提升。在整个模型中，importance就是它在所有树上importance的均值。

例如，对于一个二元分类问题，我们用了gini impurity作为分叉标准的随机森林。假设，森林中一共三棵树。

    特征A在树1中作为划分点，并且在这个点上，gini从0.6降到0.3。

    特征A在树2中作为划分点，并且在这个点上，gini从0.5降到0.4。

    特征A并没有出现在树3中。
    

    特征A的importance为：((0.6-0.3)+(0.5-0.4))/3=0.13

    特征B在树1中作为划分点，并且在这个点上，gini从0.3降到0.1。

    特征B在树2中作为划分点，并且在这个点上，gini从0.7降到0.5。

    特征B在树3中作为划分点，并且在这个点上，gini从0.4降到0.1。

    特征B的importance为：((0.3-0.1)+(0.7-0.5)+(0.4-0.1))/3=0.23



上面的例子是gini，当然也可以entropy。对于回归任务来说，损失函数可以是MSE，或者MAE之类。