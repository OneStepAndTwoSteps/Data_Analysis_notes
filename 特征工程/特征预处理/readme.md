#  数据的预处理(标准化、归一化)

通过特征提取，我们能得到未经处理的特征，这时的特征可能有以下问题：

__1、不属于同一量纲：__ 即特征的规格不一样，不能够放在一起比较。无量纲化可以解决这一问题。

__2、信息冗余：__ 对于某些定量特征，其包含的有效信息为区间划分，例如学习成绩，假若只关心“及格”或不“及格”，那么需要将定量的考分，转换成“1”和“0”表示及格和未及格。二值化可以解决这一问题。

__3、定性特征不能直接使用：__ 某些机器学习算法和模型只能接受定量特征的输入，那么需要将定性特征转换为定量特征。最简单的方式是为每一种定性值指定一个定量值，但是这种方式过于灵活，增加了调参的工作。通常使用哑编码的方式将定性特征转换为定量特征：假设有N种定性值，则将这一个特征扩展为N种特征，当原始特征值为第i种定性值时，第i个扩展特征赋值为1，其他扩展特征赋值为0。哑编码的方式相比直接指定的方式，不用增加调参的工作，对于线性模型来说，使用哑编码后的特征可达到非线性的效果。

__4、存在缺失值：__ 缺失值需要补充。

__5、信息利用率低：__ 不同的机器学习算法和模型对数据中信息的利用是不同的，之前提到在线性模型中，使用对定性特征哑编码可以达到非线性的效果。类似地，对定量变量多项式化，或者进行其他的转换，都能达到非线性的效果。

我们 __使用sklearn中的preproccessing库__ 来进行数据预处理，可以覆盖以上问题的解决方案。

## 一、无量钢化 

无量纲化使不同规格的数据转换到同一规格。常见的无量纲化方法有标准化和区间缩放法。标准化的前提是特征值服从正态分布，标准化后，其转换成标准正态分布。区间缩放法利用了边界值信息，将特征的取值区间缩放到某个特点的范围，例如[0, 1]等。

## 标准化

### 1.1、z-score标准化：

默认情况下，推荐使用zscore标准化。(主要原因是max-min标准化有一个问题，就是如果测试集里的数据特征的值出现比训练集的特征值最大值还大，或者比训练集的特征值最小值还小的情况，需要特殊处理。)

          
这是最常见的特征预处理方式，基本所有的线性模型在拟合的时候都会做 z-score标准化。具体的方法是求出样本特征x的均值mean和标准差std，然后用（x-mean)/std来代替原特征。这样特征就变成了均值为0，方差为1了 __(标准正态分布)__。 

在sklearn中，我们 __可以用StandardScaler来做z-score标准化__ 。当然，如果我们是用pandas做数据预处理，可以自己在数据框里面减去均值，再除以方差，自己做z-score标准化。    

__如：当只有一个特征时，我们进行标准化也要用双 [],即[[]]__

      ss=StandardScaler()
      train_x=data[['False']]
      train_x=ss.fit_transform(train_x)


#### 关于 z-score标准化 后应该注意    <------- 重点

如果训练模型时需要对 __训练集特征进行Z-score标准化__ ，那么测试的时候你需要使用训练集的标准化对应参数 __对测试集进行标准化__。

__比如：__ 

训练集某特征的均值是3， 标准差是4， 你做了z-score标准化，变成了均值0， 标准差1的数据，最后训练了模型。对于你的测试集的该特征，你需要减3再除以4，这样标准化后再去做预测。

#### 参考案例

-《[Z-score训练集标准化后，测试集使用相同标准标准化：案例中里面的第9部分](https://github.com/ljpzzz/machinelearning/blob/master/classic-machine-learning/regression_production_example.ipynb)》

__注意：__ 如果不是使用 z-score 做的标准化，我们那么预测集并不需要减训练集均值，除训练集标准差。

### 1.2、max-min区间缩放法：

也称为离差标准化，预处理后使特征值映射到[0,1]之间。具体的方法是求出样本特征x的最大值max和最小值min，然后用(x-min)/(max-min)来代替原特征。如果我们希望将数据映射到任意一个区间[a,b]，而不是[0,1]，那么也很简单。用(x-min)(b-a)/(max-min)+a来代替原特征即可。

在sklearn中，我们可以用MinMaxScaler来做max-min标准化。 __这种方法的问题就是如果测试集或者预测数据里的特征有小于min，或者大于max的数据，会导致max和min发生变化，需要重新计算。__

__所以实际算法中， 除非你对特征的取值区间有需求，否则max-min标准化没有 z-score标准化好用。__


### 1.3、RobustScaler标准化

如果数据 __有离群点__，上述StandardScaler效果可能不好，这种情况可以使用RobustScaler，它有对数据中心化和数据的缩放鲁棒性更强的参数。

RobustScaler根据分位数范围（默认为IQR：Interquartile Range）删除中位数并缩放数据。IQR是第1四分位数（第25个分位数）和第3个四分位数（第75个分位数）之间的范围。

通过计算训练集中样本的相关统计数据，对每个特征独立地进行居中和缩放。然后存储中间和四分位数范围以使用该transform方法用于以后的数据。

数据集的标准化是许多机器学习估计器的常见要求。通常，这通过去除均值和缩放到单位方差来完成。但是，异常值通常会以负面方式影响样本均值/方差。在这种情况下，中位数和四分位数范围通常会产生更好的结果。

## 归一化

### 1.1、Normalizer归一化：

归一化依照特征矩阵的行处理数据，其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都转化为“单位向量”。 


## 标准化和归一化的异同

### 归一化方法：

      1、把数变为（0，1）之间的小数
      主要是为了数据处理方便提出来的，把数据映射到0～1范围之内处理，更加便捷快速。

      2、把有量纲表达式变为无量纲表达式
      归一化是一种简化计算的方式，即将有量纲的表达式，经过变换，化为无量纲的表达式，成为纯量。


### 归一化带来的好处

      1.提升模型的收敛速度
      2.提升模型的精度  
      3.深度学习中数据归一化可以防止模型梯度爆炸。


### 标准化方法：

      1、标准化是通过特征的平均值和标准差，将特征缩放成一个标准的正态分布，缩放后均值为0，方差为1。但即使数据不服从正态分布，也可以用此法。
      特别适用于数据的最大值和最小值未知，或存在孤立点。

      2、标准化是为了方便数据的下一步处理，而进行的数据缩放等变换，不同于归一化，并不是为了方便与其他数据一同处理或比较。
      
      
但是我们不需要过度区分标准化和归一化，因为这块目前没有共识。所以我们只需要讨论具体的标准化或者归一化方法，比如z-score，max-min等。


## 数据规范化

-《[sklearn 中数据规范化库的使用](https://github.com/OneStepAndTwoSteps/Data_Analysis/blob/master/Sklearn%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%BA%93/metrics/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/preprocessing/%E6%95%B0%E6%8D%AE%E8%A7%84%E8%8C%83%E5%8C%96.md)》


         
### Sklearn的数据预处理模块：


      类                  功能                                   说明
    StandardScaler      无量纲化              标准化，基于特征矩阵的列，将特征值转换至服从标准正态分布

    MinMaxScaler        无量纲化              区间缩放，基于最大最小值，将特征值转换到[0, 1]区间上

    Normalizer          归一化                基于特征矩阵的行，将样本向量转换为“单位向量”

    Binarizer           二值化                基于给定阈值，将定量特征按阈值划分

    OneHotEncoder       哑编码                将定性数据编码为定量数据

    Imputer             缺失值计算             计算缺失值，缺失值可填充为均值等

    PolynomialFeatures  多项式数据转换         多项式数据转换

    FunctionTransformer 自定义单元数据转换      使用单变元的函数来转换数据函数来转换数据


## 注意

__是否进行标准化，可能会影响模型的预测分数,模型的分数可能会偏高也可能会偏低__

虽然大部分机器学习模型都需要做标准化和归一化，也有不少模型可以不做做标准化和归一化，主要是基于概率分布的模型，比如决策树大家族的CART，随机森林等。当然此时使用标准化也是可以的，大多数情况下对模型的泛化能力也有改进。


## Z-socre 补充

### 问：

在kaggle中的一些kernel中存在，直接使用 Z-score 标准化 训练集 和 测试集 的操作，这样的做法正确吗？

### 答：

kaggle里这么做，是因为他们比赛的数据集是确定的，这样做可以求出最优的均值和方差。实际生产项目里，未知数据很多，是不能这么做的。因此只能以训练集的均值和方差为准。


### 问：

为什么比赛数据集是确定的就可以将训练集和测试集直接进行Z-score标准化，而不需要对测试集进行训练集训练集相同标准化的处理呢？

在实际生产项目中，我们如果要进行分析了，我们数据不应该已经收集好了吗？那老师您说的未知数据到底是指什么呢？可以举个例子吗？

为什么未知数据多的时候，不可以直接标准化，而需要将测试集减去训练集标准化之前的均值，然后除以标准差呢？

### 答：

算法比赛的目的是为了在给定的数据集（绝大多数数据已知）上达到最大准确度。而生产项目是为了在给定的+海量未知的数据集上达到最大准确度。因此生产项目的算法泛化能力（对未知数据的预测能力）一定要强。也就是尽量不要过于依赖于已知数据集。所以我们一般只拿训练集的均值方差做zscore标准化。

算法比赛，是为了追求极限，因此可以使用一些非常规的方法达到好的比赛成绩。虽然算法比赛也会有一些未知的给定数据用于评判选手的成绩，但是这个测试数据量很小，因此影响也会小。而生产环境里未知的数据量极大，这样做并没有太多好处。


