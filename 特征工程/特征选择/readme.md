### 特征选择

特征选择( Feature Selection )也称特征子集选择( Feature Subset Selection , FSS )，或属性选择( Attribute Selection )。是指从已有的M个特征(Feature)中选择N个特征使得系统的特定指标最优化，是从原始特征中选择出一些最有效特征以降低数据集维度的过程，是提高学习算法性能的一个重要手段，也是模式识别中关键的数据预处理步骤。对于一个学习算法来说,好的学习样本是训练模型的关键。

### 如何进行特征选择

当数据预处理完成后，我们需要选择有意义的特征输入机器学习的算法和模型进行训练。通常来说，从两个方面考虑来选择特征：

__特征是否发散：__ 如果一个特征不发散，例如方差接近于0，也就是说样本在这个特征上基本上没有差异，这个特征对于样本的区分并没有什么用。

__特征与目标的相关性：__ 这点比较显见，与目标相关性高的特征，应当优选选择。除方差法外，本文介绍的其他方法均从相关性考虑。

__根据特征选择的形式又可以将特征选择方法分为3种：__

__Filter：过滤法__，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。__如：方差选择法，相关系数法，卡方检验，互信息法__

__Wrapper：包装法__，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。__如：递归特征消除法__

__Embedded：嵌入法__，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。__如：基于惩罚项的特征选择法，基于树模型的特征选择法。__

我们使用sklearn中的feature_selection库来进行特征选择。

### Filter Wrapper Embedded 对比

#### Filter
__Filter：__ 过滤式特征选择的评价标准从数据集本身的内在性质获得，与特定的学习算法无关，因此具有较好的通用性。通常选择和类别相关度大的特征或者特征子集。过滤式特征选择的研究者认为，相关度较大的特征或者 特征子集会在分类器上获得较高的准确率。过滤式特征选择的评价标准分为四种，即距离度量、信息度量、关联度度量以及一致性度量。

__优点：__ 算法的通用性强；省去了分类器的训练步骤，算法复杂性低，因而适用于大规模数据集；可以快速去除大量不相关的特征，作为特征的筛选器非常合适。

__缺点：__ 甶于算法的评价标准独立于特定的学习算法，所选的特征子集在分类准确率方面通常低于Wrapper方法。

#### Wrapper
__Wrapper：__ 封装式特征选择是利用学习算法的性能评价特征子集的优劣。因此，对于一个待评价的特征子集， Wrapper方法需要训练一个分类器，根据分类器的性能对该特征子集进行评价Wrapper方法中用以评价特征的学习算法是多种多样的，例如决策树、神经网络、贝叶斯分类器、近邻法、支持向量机等等。

__优点：__ 相对于Filter方法，Wrapper方法找到的特征子集分类性能通常更好。

__缺点：__ Wrapper方法选出的特征通用性不强，当改变学习算法时，需要针对该学习算法重新进行特征选择；由于每次对子集的评价都要进行分类器的训练和测试，所以算法计算复杂度很高，尤其对于大规模数据集来说，算法的执行时间很长。

#### Embedded
__Embedded:__ 在嵌入式特征选择中，特征选择算法本身作为组成部分嵌入到学习算法里最典型的即决策树算法，如ID3、C4.5以及CART算法等，决策树算法在树增长过程的每个递归步都必须选择一个特征，将样本集划分成较小的子集，选择特征的依据通常是划分后子节点的纯度，划分后子节点越纯，则说明划分效果越好，可见决策树生成的过程也就是特征选择的过程。

