## 推荐系统

### 定义推荐系统

接下来的我们看一个预测电影评分的案例。 

<div align=center><img src="https://raw.githubusercontent.com/OneStepAndTwoSteps/data_mining_analysis/master/static/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/1.png"/></div>



问题是：假设你在亚马逊工作，你需要让你的用户评价不同的电影。从0颗星到5颗星进行一个评定。前三部电影为爱情电影，后两部电影为动作电影

先看图中的列表，列表左侧表示电影的名称，这里我们可以看到一共有5部电影，和四个用户，这四个用户分别对这几部电影进行了一个评分，Alice非常喜欢前两部电影，他为前两部电影都打了满分5颗星，第三部电影他暂时还没有看过，所以评分我们先假设为一个问号，而且alice不喜欢后两部电影所以他给后两部电影打了0分。同理Bob，Carol，Dave分别也对这些电影进行了评分，分别打出了他们自己认为的星级。

我们从这几个评分可以看出，alice和Bob好像非常喜欢看爱情电影(给了爱情电影很高的评分)，好像不喜欢看动作电影，而carol和Dave正好相反，他们反而好像更喜欢看动作电影，不是很喜欢看爱情电影。

同时这里我们会介绍几个符号,分别就是图中的这4个符号，分别表示…（含义已经写出）

推荐系统的问题就是给出了r(I,j)和y(I,j)数据，然后去查找那些没有被评级的电影，并试图去预测这些电影的星级，我们通过上面的评分可以看出alice非常喜欢爱情电影，我们就可以假设他会给第三部电影打5颗星，Bob可能会给4.5颗，carol可能会打0颗星，Dave可能会给第三部电影打0颗星，给第五部电影打5颗星。

所以我们如果想要开发一个推荐系统，那么我们就需要一个学习算法可以去自动的为我们填补这些缺失值，这样我们就能知道哪些用户还有哪些电影没有观看，并推荐电影给他们。接下来我们会设计一个算法来解决这样的问题。


### 基于内容的推荐系统 

接下来我会讲一个建立推荐系统的方法，这个方法被称为基于内容的推荐算法。

<div align=center><img src="https://raw.githubusercontent.com/OneStepAndTwoSteps/data_mining_analysis/master/static/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/2.png"/></div>



我们如何能够预测出这些没有评星级的电影的等级呢？我们假设每一部电影都有对应一个特征集，我们假设每一部电影都有两个特征，x1和x2分别代表属于爱情片和动作片的成都，举个例子，第一部电影为例，特征x1表示它是爱情片的程度是0.9，特征x2表示是动作片的程度为0几乎没有动作内容，第二步电影它同理。

这样每一部电影都可以用一个特征向量来表示，这里我们引入一个特征变量x0，表示的就是我们之前常讲的截距,如𝑥(1)是第一部电影的特征向量为[0.9 0]。 

现在我们得到了特征向量，接下来我们将该问题看成是一个线性回归的问题，，我们可以针对每一个用户都训练一个线性回归模型，如𝜃(1)是第一个用户的模型的参数。 于是，我们有：

𝜃(𝑗)用户 𝑗 的参数向量 --- 就是我们评的星级
𝑥(𝑖)电影 𝑖 的特征向量 --- 我们的特征向量

以alice对第三部电影的评分为例，我们从右侧的特征向量中我们可以看到𝑥(3)第一部电影的特征向量 [1,0.99 0]，同时我们假设我们我们通过一个学习算法得到了他的参数值θ1为[0,5,0]，根据预测用户j对电影i的评分公式我们可以得到θ5的转置乘以x(3)得到的评分为4.95。 我们从得到的结果来看，结果还是比较合理的。

__符号对应的表示:__
<div align=center><img src="https://raw.githubusercontent.com/OneStepAndTwoSteps/data_mining_analysis/master/static/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/3.png"/></div>

__代价函数:__

<div align=center><img src="https://raw.githubusercontent.com/OneStepAndTwoSteps/data_mining_analysis/master/static/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/4.png"/></div>

其中 𝑖:𝑟(𝑖,𝑗)表示我们只计算那些用户 𝑗 评过分的电影。(𝜃(𝑗))𝑇𝑥(𝑖)表示我们的预测评分，y(𝑖,j)是真实评分。 相减表示我们的误差，在一般的线性回归模型中，误差项和正则项应该都是乘以1/2𝑚，在这里我们将𝑚去掉，因为m是一个常数，删除他队结果没有影响。并且我们不对方差项𝜃0进行正则化处理(k从1开始到n)。 


__优化目标:__

<div align=center><img src="https://raw.githubusercontent.com/OneStepAndTwoSteps/data_mining_analysis/master/static/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/5.png"/></div>

在单个用户求电影参数是，我们的最小化目标就是这个关于θj的优化对象。但是在实际的推荐系统中我们不仅仅只对单个用户进行一个求参数，我们还需要对多个用户进行电影参数的求解。

在多用户的参数求解中我们需要求θ(1)到θ(nu) nu表示我们用户的个数。

__总结：__

<div align=center><img src="https://raw.githubusercontent.com/OneStepAndTwoSteps/data_mining_analysis/master/static/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/6.png"/></div>

我们的优化目标函数就是要去求我们的J(θ(1), θ(2),.... θ(nu))

如果我们要用梯度下降法来求解最优解，我们计算代价函数的偏导数后得到梯度下降的更新公式为：上面的式子，同时我们的梯度下降更新中分为两种情况，一种是k=0，一种是k不等于0，分别表示的是我们的正则化项中是否需要正则化我们的θ(0)。

这里的梯度下降优化和我们的线性回归是相似的，不同的是我们这里舍去了1/m，因为它是一个常数，舍去对结果没有影响

以上就是我们如何使用线性回归来学习我们的用户参数θ，来预测我们的电影评分，以上这种方法就是我们的基于内容的推荐算法，因为我们假设的特征变量是已有的不同电影的各个特征，既我们描述的电影内容的特征量，这个电影的属于爱情电影的程度，或者属于动作电影的程度。

但是对于一些电影是没有这样的特征向量，或者说很难提取出这样的特征量，下面我们会讲另外一种推荐算法，这种算法不基于我们的特征量。


### 协同过滤

在之前的基于内容的推荐系统中，对于每一部电影，我们需要先得到它其中的特征量，然后使用这些特征训练出了每一个用户的参数。但是有一些电影的特征我们难以自行进行提取，但我们在使用协调过滤算法时，其有一个特点，它会自行去学习所要使用的特征。

<div align=center><img src="https://raw.githubusercontent.com/OneStepAndTwoSteps/data_mining_analysis/master/static/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/7.png"/></div>


之前我们说过，有一些电影的特征，难以去获取，同时我们进行预测时使用的特征不仅仅只有两个，可能还需要其他的特征，那么我们如何获取这些特征呢？

换一个问题，假如我们有一个数据集，但是我们不知道这些特征值是多少，比如我们得到了一些关于电影的数据即不同用户对电影的评分，但是我们不知道具体的属于爱情还是动作电影的程度所以我们现在使用问号来表示。

假设现在我们采访了一些用户，并且每一个用户都告诉了我们他们喜欢爱情电影的程度，以及喜欢动作电影的程度，这样alice就有了对应的参数θ(1),假设alice告诉我们他非常喜欢爱情电影那么他对应的值就是5，假设alice告诉我们他非常不喜欢动作电影那么他对应的值就是最后的那个0，Bob的喜好和Alice相同，我们也得到了一个相同的参数向量，Carol告诉我们他非常喜欢动作电影，那么θ3的最后一个值就是5，假设Carol不喜欢爱情电影那么，第二个对应值就是0，Dave同理

假设现在每个用户都告诉我们他们各自的θ(j)是多少，这就是表明了他们对不同题材的电影的喜欢程度，如果我们可以从用户那里得到这些参数θ的值，那么理论上我们就可以预测每一步电影的x1和x2的值，什么意思呢？假设我们现在不知道第一部电影到底是爱情电影还是动作电影，但是我们现在可以根据用户的喜好进行判断，我们通过采访可以知道Alice用户和Bob用户喜欢爱情电影，不喜欢动作电影，Carol用户和Dave用户喜欢动作电影，不喜欢爱情电影，现在我们看评分，我们可以看出Alice用户和Bob对这一部电影的评分很高，但是Carol用户和Dave用户对其评分很低，那么我们基本上就可以推断出这部电影是爱情电影，不大可能是动作电影，此时我们基本上可以假设我们的x1=1，x2=0

在数学上我们可以表示成(θ(1))Tx(1)约等于5，(θ(2))Tx(1)约等于5，(θ(3))Tx(1)约等于0，(θ(4))Tx(1)约等于0，此时我们得到我们的x(1)的特征向量为[1，1，0]，这样我们就得到了这些特征值，则又可以用于预测我们的分数。

__优化目标:__

<div align=center><img src="https://raw.githubusercontent.com/OneStepAndTwoSteps/data_mining_analysis/master/static/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/8.png"/></div>

假设我们的用户告诉了我们他的偏好，也就是说用户已经给我们提供了θ(1)到θ(nu)的值，此时我们要学习的是电影i的特征向量x(i),我们能做的就是提出下列的优化问题，所以需要把所有的指数j相加，得到对电影i的评分，因为我们想要得到电影i的特征，也就是向量x(i),为了使我们的预测值更加准确，我们需要保证我们的 (𝜃(𝑗))𝑇𝑥(𝑖) – y(i,j) 尽可能的小，后面我们加了一个正则项，防止我们特征值过大，这就是如何从一部特定的电影之中学习到特征的方法。

但是我们要做的是我们要学习出所有电影的所有特征(第二个式子)，所以我们现在要做的是，在上面两处箭头处加上了两个求和项，因为我要对所有电影进行求和，nu个电影，然后最小化上边这个目标函数，如果将其最小化，此时我们就可以得到合适的所有电影的特征。

__总结：__

在上面我们讲到如果你有所有电影评分的集合，即θ(i,j)和y(i,j)你有这些评分数据。

于是你可以根据不同的电影特征，我们可以学习参数θ，如果你已知这些特征，就可以学习出不同的用户的参数θ。

我们刚刚讲的是，如果你的用户愿意为你提供这些θ参数，你就能估计出各种电影的特征，即如果已知θ我们可以求x，即已知x我们可以求θ，我们实际上做的就是，随机的猜取一些θ值，在有了这些随机初始化的θ值之后，我们就可以学习出不同电影的特征，通过x我们可以优化我们的θ，这样我们就有了更好估计用户的θ值，通过这些θ值我们又可以获的更好的特征，以此类推。
      
      更新    更新     更新    更新
    θ ----> x ----> θ ----> x ----> θ .......

通过上述的这种方法我们就可以将算法收敛到一组合理的电影特征，以及一组合理的对不同用户参数的估计，这就是基本的协同过滤算法，但是这实际上不是我们最后要去适应的算法，在下面我们会讲如何改进这个算法，让其更高效的进行运行，我希望通过这些让你们明白如何规划处一个问题，能让你同时用这些电影中学习处参数和特征，对应这种推荐系统的问题，该问题建立在每一个用户都对电影进行了评价 ，并且每一部电影都被数位用户评价过的情况下，这样你才能重复这个迭代过程，来估计θ和x

协同过滤算法就是当你在执行算法时，要观察大量的用户，观察这些用户的实际行为，来协同得到更佳的每一个人对电影的评分值，因为如果每一个用户都对一部分电影进行了评价，那么每一个用户都在帮助算法，学习出更好的特征，也就是说通过自己，对几部电影的评价，我就可以帮助系统更好的学习特征，然后通过这些特征，有可以用来更好的预测其他用户的评分，协同的另一个意思，是说，每一位用户都在帮助算法，更好的进行特征学习，这就是协同过滤

### 协同过滤算法

首先如果给你几个特征表示电影，你可以用它来学习用户的参数θ。

第二如果给你用户参数，你可以用它来学习电影的特征。

__接下来我们会将这两个概念进行结合，得到一个新的协同过滤算法__

<div align=center><img src="https://raw.githubusercontent.com/OneStepAndTwoSteps/data_mining_analysis/master/static/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/9-1.png"/></div>

前面我们说过通过特征向量我们可以学到我们的用户参数θ，同时我们也说过，通过我们的用户参数θ，我们也可以学习我们的特征向量，此时我们可以做的就是通过不断的计算，或许是通过随机的初始化这些参数然后解出θ，然后通过θ解出我们的电影特征，然后通过电影特征再更新我们的用户参数θ，然后不断迭代。

实际上还有一种更好的方法，我们不需要再像这样进行迭代，而是可以将x和θ共同的计算出来，下面我就会讲这样的算法，我们要做的就是将两个优化目标函数，给合为一个函数。

所以我现在定义一个新的优化目标函数J，它是一个代价函数，是关于特征x和参数θ的函数，它其实就是上面那两个优化目标函数

爲了能把这个式子解释清楚，我们先看前两个式子中的平方误差项(就是紫色框框起来的部分)是相同的，我们先看一下他们到底在做什么，第一个求和运算是所有用户j的总和(j=1)，意思为对该用户评分的所有电影求和。和所有被该用户评价过电影的总和(i:r(i,j)=1)，所有的(i,j）对全加起来，每项对应被某一用户评分过的某一电影，

第二个运算求和，表示对于每部电影i，将所以对他评分过的用户j求和，这两个求和运算，都是对所有的r(i,j)=1的(i,j)对求和，就是对所有评分的用户-电影对进行求和，因此这两个式子，其实就是这里的第一项，我们已经给出了这个求和式子，这里写着所有的r(i,j)值为1的(i,j)对求和，我们要做的就是定义一个我们想将其最小化的，合并之后的优化目标函数，让我们可以同时解出x和θ，优化目标函数里另一些项是θ所进行的正则化(就是红色和蓝色框框起来的部分)。


这个合并函数(优化目标函数J)有一个有趣的地方是如果你假设x为常数，并关于θ优化的话，你其实就是在计算第一个式子，反过来也一样，如果你把θ作为常量，然后关于x求J的最小值的话，那么其实就是在计算第二个式子，因为如果只关于x或θ进行最小化运算的话，后面这两项，会有一项变为常数，所以这里的优化目标是关于x和θ的两个代价函数合并起来，为了提出一个综合优化目标问题，我们所要做的就是将这个代价函数(橙色框框起来的部分)视为特征x和用户参数θ的函数，对他整体最小化，作为一个既关于x也关于θ的函数，这和前面的算法唯一不同的是不需要反复的计算，就像我们之前所说先关于θ最小化，然后关于x最小化，然后再关于θ最小化，然后再关于x最小化，在新的版本中我们不需要不断的在x和θ这两个参数之间不断的迭代，我们所要做的是关于这两组参数同时进行最小化(绿色框部分)。

最后一件事，当我们以这样的方法学习特征量时，之前我们遵循的惯例是我们所使用的特征x0=1(对应一个截距项)当我们以这种形式真正去学习特征量时，我们不再遵循这一惯例，这些我们讲学习的特征量x是n维实数，而先前我们所有的特征值x是n+1维，包括截距项，删除掉x0我们现在只有n维的x。

同样的因为参数θ具有相同的维度，所有θ也是n维的，因为如果没有x0，那么θ0也不再需要，我们放弃这一惯例的理由是，我们现在是在学习所有的特征，我们没有必要将一个特征值硬编码为1，因为如果算法真的需要一个特征永远为1，它可以选择靠自己去获取1这个数值，如果算法需要的话，它可以将x1设置为1，所以没有必要将这个特征定为1定死，现在算法可以灵活的自行学习。

__小结:__

<div align=center><img src="https://raw.githubusercontent.com/OneStepAndTwoSteps/data_mining_analysis/master/static/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/10.png"/></div>

把所有的讲的这些结合起来就是我们的协同过滤算法，首先我们将会把x和θ初始化为小的随机数，这有点像神经网络的训练，我们也是将所有的神经网络的参数用小的随机数值来初始化。

接下来我们要用梯度下降把这个代价函数最小化，如果你进行求导的话你会发现梯度下降算法写出来的更新式子是这样的(红色框框起来的部分)，就是关于特征值x(i)k的偏微分。第二个式子也就是关于我们正在最小化的参数θ所作的偏微分，这里再提醒一下，在式子中我们不再有关于1对应x0项，所以x是n维实数θ也是n维实数，在这个表达式里面我们将所有的参数θ和x做正则化，不存在θ0这种需要特别正则化的特殊情况，或者说跟θ1到θn的正则化，这就是为什么在式子中没有分出k=0的特殊情况。

然后我们使用梯度下降最小化我们的代价函数J，关于特征x和参数θ。

<div align=center><img src="https://raw.githubusercontent.com/OneStepAndTwoSteps/data_mining_analysis/master/static/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/11.png"/></div>


1.最后给你一个用户，如果这些用户具有一些参数θ，以及给你一部电影关于已知的特征x，我们可以预测该用户对电影的评分，及θ的转置乘以x。

2.补充：如果用户j尚未对电影i评分，我们可以预测，这个用户j会根据θ(j)的转置乘以x(i)，这就是预测出的对电影i的评分。

3.所以这就是协同过滤算法，你使用这个算法几乎所有的电影的特征和所有的用户参数能对用户会如何对他们尚未评分的电影做出评分给出相等准确的预测。


### 低秩矩阵分解 

接下来我们会讨论协同过滤算法的向量化实现。另外在介绍一下你使用算法可以实现的一些功能，比如给定一个商品，你可以找到其他相关的商品，接下来我们看如何解决这样的问题。

<div align=center><img src="https://raw.githubusercontent.com/OneStepAndTwoSteps/data_mining_analysis/master/static/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/12.png"/></div>

这是我们之前举得电影的例子，我们现在要做的是得到所有的用户对电影的评分，然后将其写入到右侧的矩阵，这里我们有四个用户5部电影，所以矩阵的第(i,j)个元素就是y(i,j)，第j个用户对第i部电影的评分



<div align=center><img src="https://raw.githubusercontent.com/OneStepAndTwoSteps/data_mining_analysis/master/static/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/13.png"/></div>

我们之前说评分可以使用(𝜃(𝑗))𝑇𝑥(𝑖) 来进行预测，所以左边这个评分我们可以使用右侧的矩阵进行表示，相同框的颜色的评分互相对应，以此类推分别对应。

现在给定这个预测评分矩阵，有一个比较简单的向量化的方法来写出他们。

比如我们定义一个矩阵X，里面的都是我们的特征量x1到xn的转置，我们再定义一个矩阵O，里面都是我们的用户参数θ1到θn的转置，我们通过 X * O的转置，我们就可以得到我们的预测评分，这里有一个公式AT* B=BT * A ,所以这个协同过滤算法还有另外一个名字，就是低秩矩阵分解，实际上当你听到别人在讲低秩矩阵分解其实讲的就是协同过滤算法

<div align=center><img src="https://raw.githubusercontent.com/OneStepAndTwoSteps/data_mining_analysis/master/static/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/14.png"/></div>

最后在我们运行协同过滤算法之后，我们如何利用已经学习到的属性来找相关的电影。

具体的说，对于每一部电影，每一个商品我们已经获取到了其特征量x，在之前你看不知道要选取什么属性作为我们的特征，但是通过运行这个算法我们将会自行捕捉到一些重要的特征。

比如我们对于每一部电影i，我们都学习到一个特征向量X，特征向量里面包含了不同的特征值，比如这个电影属于爱情电影的程度，属于动作电影的程度，属于喜剧电影的程度…

那我们如何寻找到相关星比较大的电影呢(也就是如何找相似度较大的电影呢？) 这里我们可以通过计算这两个电影向量之间的距离，如果距离小，相似度就高。我们将相似度高的电影对用户进行一个推荐就好了。如果我们西药给用户推荐5部电影，我们就选取5部相似度就高的，也就是特征距离最小的5部电影即可。



### 均值归一化 

算法最后实现过程中的细节，即均值归一化，有时它可以让算法运行的更加的好。


<div align=center><img src="https://raw.githubusercontent.com/OneStepAndTwoSteps/data_mining_analysis/master/static/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/15.png"/></div>

我们沿用之前的电影评分的例子，不过在之前的基础上我们增加了一个用户Eve，但是Eve没有做任何的电影评价，那么我们的算法会对这个用户做什么呢？

假设n=2，我们要学习两个特征变量，我们现在要学习用户参数θ5，当我们看优化目标第一项(红色框部分)因为Eve没有对电影做过评价所以，没有电影满足R(i,j)=1这个条件，所以这一项完全不影响(红色框)，所以蓝色框的内容是唯一影响θ5的唯一项，这就是说，我们想选一个向量θ5，使得最后的正则化项尽可能的小，换句话说，我们想要最小化这个式子，我们最终会得到θ5=[0,0]因为正则化项会让你的参数接近0，如果没有数据能够使参数远离0，我们就会得到θ5=零向量这个结果，所以当我们预测用户5会如何给电影打分，我们有θ5转置乘以x(i)，对于任意i结果都会等于0，因为对任意值θ5都是0，因此我们会得到w吗预测的Eve给所有电影的都是0颗星，但是这个结果似乎看起来没有什么用，比如第一个电影有两个人给他评了5颗星，第五个电影同样有人给他评了5颗星，有些人确实是喜欢这些电影，所以预测出Eve对电影的评分为0颗星是没有用的，但实际上我们如果预测出Eve对电影的评分为0颗星，我们还是没有任何办法推荐电影给他，因为所有的电影对于Eve的评分是一样的，所以没有一部电影拥有高一点的预测评分，让我们能推荐给他，所以这种结果不太好。但是均值归一化的方法可以帮助我们解决这样的问题，下面我们介绍它是如何工作

和以前一样我们江评分都放在矩阵Y中，就是把这些评分全部都整合再矩阵Y中，矩阵的最右边为Eve的评分，因为没有评分所以都是问号。


<div align=center><img src="https://raw.githubusercontent.com/OneStepAndTwoSteps/data_mining_analysis/master/static/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/16.png"/></div>

我要做的就是计算每个电影所得的均值，我要把它们存在一个叫μ的向量中，所以第一个电影得到了两颗五星，和两颗0星的评价，均值就是2.5星，而第二个电影的评价同理也是2.5星等等。

我们要做的是观察这些电影评分，现在我要减去均分，所以这第一个元素5减去2.5等于2.5，第二个元素5-2.5=2.5 第三个元素0-2.5=-2.5.

此时我们的到的Y矩阵中评价评分都是0(比如2.5+2.5+-2.5+-2.5=0)，接下来我们要做的就是对这个评分数据集使用协调过滤算法，所以我要假设，这就是我从用户那里得到的数据，或者假设他们就是我从用户那里得到的实际评分，我要把Y当作是我的数据集，用它来学习我的参数θ(j)和特征x(i)，就是用这些均值归一化后的电影评分来学习，当我想做电影评分预测时，我要做的步骤如下：

对用户j对电影i的评分，我要预测他为θj的转置乘以x(i),其中x和θ都是从均值归一化的数据集中学习出来的参数，但是因为我已经对数据集减去了均值，所以为了会给电影i预测评分，我要把这个均值加回来，所以我要再加回μi，所以这就是我们得到的预测值，因为训练数据减去了所有的均值，所以当我们进行预测时我们需要给电影加上这个均值μi，所以对于用户5即Eve，之前幻灯片中的问题，在这里仍然存在，Eve从来没有给任何电影打分，所以学习到的用户5的参数，仍然还是会等于[0,0]，所以我们会得到的是对于特定的电影i，我们预测Eve的评分是θ(5)的转置乘以特征x(i)再加上μi，即使θ(5)的转置乘以特征x(i)等于0，我们对电影的评价也会是μi，这实际上是有意义的，这样我们对应Eve会预测出他第一个电影的评分为2.5，第二部电影的评分为2.5，第三部为2，第四部位2.25，第五部位1.25.

意思是当我们Eve没有对电影进行评分时，我们采用均值(平均分)进行填充。

这里补充一下，这里我们讲了均值归一化，我们归一化了y的每一行，每行的均值都是0，如果有些电影没有评分，这种情况类似于没有进行过评价的用户，但是如果你有些电影是没有评分的你可以尝试这个算法的其他版本，你可以对不同的列，进行归一化使得他们的均值为0，而不是把均值归一化，说实话，如果你这些电影没有被评分你就不应该对其进行一个推荐，所以关注没有评价电影的用户比那些没有评价的电影来的更重要

__最后总结一下，这就是均值归一化的实现，它作为协同过滤算法的预处理步骤根据不同的数据集，他有时能让你的算法表现的更好些__

